{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros Iniciales: 20867 , Atributos Iniciales: 11\n",
      "root\n",
      " |-- F1: double (nullable = true)\n",
      " |-- F2: double (nullable = true)\n",
      " |-- F3: double (nullable = true)\n",
      " |-- F4: double (nullable = true)\n",
      " |-- F5: double (nullable = true)\n",
      " |-- F6: double (nullable = true)\n",
      " |-- F7: double (nullable = true)\n",
      " |-- F8: double (nullable = true)\n",
      " |-- F9: double (nullable = true)\n",
      " |-- F10: double (nullable = true)\n",
      " |-- Author: string (nullable = true)\n",
      "\n",
      "Cantidad de Nulos en cada atributo\n",
      "   F1  F2  F3  F4  F5  F6  F7  F8  F9  F10  Author\n",
      "0   0   0   0   0   0   0   0   0   0    0       0\n",
      "Descripcion de los atributos\n",
      "+-------+--------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "|Summary|                  F1|                 F2|                  F3|                  F4|                  F5|\n",
      "+-------+--------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "|  count|               20867|              20867|               20867|               20867|               20867|\n",
      "|   mean|-3.30665647008743...|0.01849807293813218|0.002328867398284...|1.154239229405380...|5.697992049061326E-8|\n",
      "| stddev|  1.0000072509694418|  2.853116758649088|  1.0582030864294198|  0.9999974852633741|  0.9999948173431626|\n",
      "|    min|           -3.498799|          -2.426761|           -3.210528|           -5.440122|           -4.922215|\n",
      "|    max|           11.819916|              386.0|                50.0|            3.987152|            1.066121|\n",
      "+-------+--------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "None\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "|Summary|                  F6|                  F7|                  F8|                  F9|                F10|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "|  count|               20867|               20867|               20867|               20867|              20867|\n",
      "|   mean|0.002539710356064...|0.003977167776872577|2.816657880866039E-5|0.002108021948531...|6.93842430632189E-5|\n",
      "| stddev|  1.0651787221451638|    1.15332490347375|   1.000003117638661|  1.0453623588623464| 1.0000098794516241|\n",
      "|    min|           -7.450257|          -11.935457|           -4.247781|           -5.486218|          -6.719324|\n",
      "|    max|                53.0|                83.0|           13.173081|                44.0|          11.911338|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import isnan, when, count, col, explode, array, lit\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.classification import LogisticRegression,OneVsRest\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import dayofweek\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.sql.functions import isnan, when, count, col, lit, sum\n",
    "from pyspark.sql.functions import (to_date, datediff, date_format,month)\n",
    "#Se carga el conjunto de datos\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Avila\").config(\"spark.some.config.option\",\"some-value\").getOrCreate()\n",
    "data = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\", \"true\").load(r\"avila.csv\")\n",
    "\n",
    "#####################################################################################################\n",
    "#PRIMER PUNTO\n",
    "#DESCRIPCION DEL CONJUNTO DE DATOS INICIAL\n",
    "#Imprime la cantidad de registros y atributos respectivamente\n",
    "print(\"Registros Iniciales:\",data.count(),\", Atributos Iniciales:\",len(data.columns))\n",
    "\n",
    "#Tipo de los atributos\n",
    "data.printSchema()\n",
    "\n",
    "# Se revisa si existen nulos en alguno de los atributos del dataset\n",
    "print(\"Cantidad de Nulos en cada atributo\")\n",
    "print(data.select([count(when(isnan(c),c)).alias(c) for c in data.columns]).toPandas().head())\n",
    "\n",
    "#Descripcion de los atributos\n",
    "print(\"Descripcion de los atributos\")\n",
    "print(data.describe().select(\"Summary\",\"F1\",\"F2\",\"F3\",\"F4\",\"F5\").show())\n",
    "print(data.describe().select(\"Summary\",\"F6\",\"F7\",\"F8\",\"F9\",\"F10\").show())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribucion del atributo clasificador\n",
      "+------+-----+\n",
      "|Author|count|\n",
      "+------+-----+\n",
      "|     F| 3923|\n",
      "|     E| 2190|\n",
      "|     B|   10|\n",
      "|     Y|  533|\n",
      "|     D|  705|\n",
      "|     C|  206|\n",
      "|     A| 8572|\n",
      "|     X| 1044|\n",
      "|     W|   89|\n",
      "|     G|  893|\n",
      "|     I| 1663|\n",
      "|     H| 1039|\n",
      "+------+-----+\n",
      "\n",
      "LIMPIEZA DE LOS DATOS\n",
      "Datos Demasiado Atipicos de F2 Eliminados: 20866\n",
      "Conversion de atributos categoricos a numericos\n",
      "+---------+-----+\n",
      "|AuthorNum|count|\n",
      "+---------+-----+\n",
      "|      8.0|  533|\n",
      "|      0.0| 8571|\n",
      "|      7.0|  705|\n",
      "|      1.0| 3923|\n",
      "|      4.0| 1044|\n",
      "|     11.0|   10|\n",
      "|      3.0| 1663|\n",
      "|      2.0| 2190|\n",
      "|     10.0|   89|\n",
      "|      6.0|  893|\n",
      "|      5.0| 1039|\n",
      "|      9.0|  206|\n",
      "+---------+-----+\n",
      "\n",
      "Conjunto Balanceado\n",
      "+---------+-----+\n",
      "|AuthorNum|count|\n",
      "+---------+-----+\n",
      "|      8.0| 2132|\n",
      "|      0.0| 6636|\n",
      "|      7.0| 2115|\n",
      "|      4.0| 2088|\n",
      "|     11.0| 1700|\n",
      "|      3.0| 1663|\n",
      "|      2.0| 2190|\n",
      "|     10.0| 1513|\n",
      "|      6.0| 1786|\n",
      "|      5.0| 2078|\n",
      "|      9.0| 1648|\n",
      "+---------+-----+\n",
      "\n",
      "Numero de Registros Dataset Limpio: 25549 , Atributos: 11\n"
     ]
    }
   ],
   "source": [
    "#Se verifica la correlacion entre los atributos\n",
    "#pd = data.toPandas()\n",
    "#print(\"Correlacion entre atributos\")\n",
    "#print(pd.corr())\n",
    "\n",
    "#Distribucion del atributo clasificador\n",
    "print(\"Distribucion del atributo clasificador\")\n",
    "data.groupby(\"Author\").count().show()\n",
    "#####################################################################################################\n",
    "#COMIENZA EL SEGUNDO PUNTO\n",
    "#LIMPIEZA DE LOS DATOS\n",
    "\n",
    "#Como se puede ver en los diagramas de cajas, el atributo F2 tiene datos que son demasiado atipicos\n",
    "#Estos registros se eliminaran\n",
    "print(\"LIMPIEZA DE LOS DATOS\")\n",
    "data = data.filter(data.F2<350)\n",
    "print(\"Datos Demasiado Atipicos de F2 Eliminados:\",data.count())\n",
    "\n",
    "print(\"Conversion de atributos categoricos a numericos\")\n",
    "indexer = StringIndexer(inputCol=\"Author\", outputCol=\"AuthorNum\")\n",
    "data = indexer.fit(data).transform(data)\n",
    "data = data.drop('Author')\n",
    "data.groupby(\"AuthorNum\").count().show()\n",
    "\n",
    "#Se balancea cada categoria (Entre 1000 y 2000 atributos cada una)\n",
    "A = data.filter(data.AuthorNum == 0.0).sample(fraction=0.24)\n",
    "F = data.filter(data.AuthorNum == 0.0).sample(fraction=0.53)\n",
    "E = data.filter(col(\"AuthorNum\") == 2.0).withColumn(\"dummy\", explode(array([lit(x) for x in range(1)]))).drop('dummy')\n",
    "I = data.filter(col(\"AuthorNum\") == 3.0).withColumn(\"dummy\", explode(array([lit(x) for x in range(1)]))).drop('dummy')\n",
    "X = data.filter(col(\"AuthorNum\") == 4.0).withColumn(\"dummy\", explode(array([lit(x) for x in range(2)]))).drop('dummy')\n",
    "H = data.filter(col(\"AuthorNum\") == 5.0).withColumn(\"dummy\", explode(array([lit(x) for x in range(2)]))).drop('dummy')\n",
    "G = data.filter(col(\"AuthorNum\") == 6.0).withColumn(\"dummy\", explode(array([lit(x) for x in range(2)]))).drop('dummy')\n",
    "D = data.filter(col(\"AuthorNum\") == 7.0).withColumn(\"dummy\", explode(array([lit(x) for x in range(3)]))).drop('dummy')\n",
    "Y = data.filter(col(\"AuthorNum\") == 8.0).withColumn(\"dummy\", explode(array([lit(x) for x in range(4)]))).drop('dummy')\n",
    "C = data.filter(col(\"AuthorNum\") == 9.0).withColumn(\"dummy\", explode(array([lit(x) for x in range(8)]))).drop('dummy')\n",
    "W = data.filter(col(\"AuthorNum\") == 10.0).withColumn(\"dummy\", explode(array([lit(x) for x in range(17)]))).drop('dummy')\n",
    "B = data.filter(col(\"AuthorNum\") == 11.0).withColumn(\"dummy\", explode(array([lit(x) for x in range(170)]))).drop('dummy')\n",
    "\n",
    "#Se juntan todas las categorias balanceadas\n",
    "data = A.union(B).union(C).union(D).union(E).union(F).union(G).union(H).union(I).union(W).union(Y).union(X)\n",
    "\n",
    "print(\"Conjunto Balanceado\")\n",
    "data.groupby(\"AuthorNum\").count().show()\n",
    "print(\"Numero de Registros Dataset Limpio:\",data.count(),\", Atributos:\",len(data.columns))\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################################\n",
    "#COMIENZA PUNTO 3\n",
    "#Entrenamiento de modelos:\n",
    "cols=data.columns\n",
    "cols.remove(\"AuthorNum\")\n",
    "assembler = VectorAssembler(inputCols=cols,outputCol=\"features\")\n",
    "\n",
    "\n",
    "#Se crea el conjunto de entrenamiento y test\n",
    "train, test = data.randomSplit([0.7, 0.3],seed=20)\n",
    "#train=assembler.transform(train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTRENAMIENTO:\n",
      "Numero de Registros Train: 17755 , Atributos: ['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'AuthorNum']\n",
      "TEST:\n",
      "Numero de Registros Test: 7794 , Atributos: 11\n"
     ]
    }
   ],
   "source": [
    "print(\"ENTRENAMIENTO:\")\n",
    "print(\"Numero de Registros Train:\",train.count(),\", Atributos:\",train.columns)\n",
    "print(\"TEST:\")\n",
    "print(\"Numero de Registros Test:\",test.count(),\", Atributos:\",len(test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creación baches\n",
    "num_baches = 4\n",
    "div = 1/num_baches\n",
    "df1,df2,df3,df4 = test.randomSplit([div,div,div,div],seed=20)\n",
    "df1.toPandas().to_csv('test/1.csv',index=False)\n",
    "df2.toPandas().to_csv('test/2.csv',index=False) \n",
    "df3.toPandas().to_csv('test/3.csv',index=False) \n",
    "df4.toPandas().to_csv('test/4.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler,VectorSizeHint\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "cols=data.columns\n",
    "cols.remove(\"AuthorNum\")\n",
    "assembler = VectorAssembler(inputCols=cols,outputCol=\"features\")\n",
    "rf = RandomForestClassifier(labelCol=\"AuthorNum\", featuresCol=\"features\",numTrees=10,subsamplingRate=1,maxDepth=10)\n",
    "pipeline = Pipeline(stages=[assembler,rf])\n",
    "\n",
    "pipelineModel = pipeline.fit(train)\n",
    "pipelineModel.write().overwrite().save(\"Spipeline_2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o897.save.\n: java.io.IOException: Path MyPipeline already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:683)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:176)\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:171)\n\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7b9867818682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipelineModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MyPipeline\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/util.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path should be a basestring, got type %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o897.save.\n: java.io.IOException: Path MyPipeline already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:683)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:176)\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:171)\n\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
     ]
    }
   ],
   "source": [
    "pipelineModel.write().save(\"MyPipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "my_pipe = PipelineModel.load(\"Spipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
