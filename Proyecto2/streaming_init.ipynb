{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----WORKING ON BATCH----\n",
      ".................\n",
      "# REGISTROS EN BATCH: 1981 , Atributos: 11\n"
     ]
    },
    {
     "ename": "StreamingQueryException",
     "evalue": "An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/home/leonardo/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\", line 2442, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File \"/home/leonardo/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\", line 207, in call\n    raise e\n  File \"/home/leonardo/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\", line 204, in call\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\n  File \"<ipython-input-1-44fd6459e7e3>\", line 77, in train_df\n    get_cluster(df)\n  File \"<ipython-input-1-44fd6459e7e3>\", line 67, in get_cluster\n    \":Paginas\").save()\n  File \"/home/leonardo/anaconda3/lib/python3.7/site-packages/pyspark/sql/readwriter.py\", line 825, in save\n    self._jwrite.save()\n  File \"/home/leonardo/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/home/leonardo/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\", line 128, in deco\n    return f(*a, **kw)\n  File \"/home/leonardo/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\", line 328, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o211.save.\n: java.lang.NoClassDefFoundError: org/apache/spark/sql/sources/v2/ReadSupport\n\tat java.lang.ClassLoader.defineClass1(Native Method)\n\tat java.lang.ClassLoader.defineClass(ClassLoader.java:756)\n\tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n\tat java.net.URLClassLoader.defineClass(URLClassLoader.java:468)\n\tat java.net.URLClassLoader.access$100(URLClassLoader.java:74)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:369)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:363)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:362)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$3(DataSource.scala:653)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:653)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:733)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:967)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:304)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.sql.sources.v2.ReadSupport\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\t... 28 more\n\n\n=== Streaming Query ===\nIdentifier: [id = 09f32c8c-9552-463f-8857-f2513ff3b7f7, runId = af974324-10b3-49f7-bf76-957e1999c9a0]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {FileStreamSource[file:/media/leonardo/HardDisk1/8/Big data/Proyects/Proyectos-Big-data/Proyecto2/test]: {\"logOffset\":0}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nFileStreamSource[file:/media/leonardo/HardDisk1/8/Big data/Proyects/Proyectos-Big-data/Proyecto2/test]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-44fd6459e7e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstreamingDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforeachBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/home/leonardo/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\", line 2442, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File \"/home/leonardo/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\", line 207, in call\n    raise e\n  File \"/home/leonardo/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\", line 204, in call\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\n  File \"<ipython-input-1-44fd6459e7e3>\", line 77, in train_df\n    get_cluster(df)\n  File \"<ipython-input-1-44fd6459e7e3>\", line 67, in get_cluster\n    \":Paginas\").save()\n  File \"/home/leonardo/anaconda3/lib/python3.7/site-packages/pyspark/sql/readwriter.py\", line 825, in save\n    self._jwrite.save()\n  File \"/home/leonardo/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/home/leonardo/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\", line 128, in deco\n    return f(*a, **kw)\n  File \"/home/leonardo/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\", line 328, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o211.save.\n: java.lang.NoClassDefFoundError: org/apache/spark/sql/sources/v2/ReadSupport\n\tat java.lang.ClassLoader.defineClass1(Native Method)\n\tat java.lang.ClassLoader.defineClass(ClassLoader.java:756)\n\tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n\tat java.net.URLClassLoader.defineClass(URLClassLoader.java:468)\n\tat java.net.URLClassLoader.access$100(URLClassLoader.java:74)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:369)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:363)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:362)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$3(DataSource.scala:653)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:653)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:733)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:967)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:304)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.sql.sources.v2.ReadSupport\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\t... 28 more\n\n\n=== Streaming Query ===\nIdentifier: [id = 09f32c8c-9552-463f-8857-f2513ff3b7f7, runId = af974324-10b3-49f7-bf76-957e1999c9a0]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {FileStreamSource[file:/media/leonardo/HardDisk1/8/Big data/Proyects/Proyectos-Big-data/Proyecto2/test]: {\"logOffset\":0}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nFileStreamSource[file:/media/leonardo/HardDisk1/8/Big data/Proyects/Proyectos-Big-data/Proyecto2/test]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /home/leonardo/Desktop/neo4j-connector-apache-spark_2.11-4.0.0.jar pyspark-shell'\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import TimestampType, StringType, StructType, StructField\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.streaming import StreamingContext\n",
    "from operator import attrgetter\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from py2neo import Graph\n",
    "\n",
    "\n",
    "graph = Graph(host=\"localhost\",password=\"unodos1234\")\n",
    "#cypher = graph.cypher()\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.ml import PipelineModel\n",
    "#Se carga el modelo que previamente se entreno para luego usarlo en la evaluacion de los streams de data\n",
    "StreamPipeline = PipelineModel.load(\"Spipeline_2\")\n",
    "\n",
    "\n",
    "#Se define el esquema de los datos que se leeran en el stream\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Avila-stream\").config(\"spark.some.config.option\",\"some-value\").getOrCreate()\n",
    "schema = \"F1 DOUBLE, F2 DOUBLE, F3 DOUBLE, F4 DOUBLE, F5 DOUBLE, F6 DOUBLE, F7 DOUBLE, F8 DOUBLE, F9 DOUBLE, F10 DOUBLE, AuthorNum DOUBLE\"\n",
    "\n",
    "#Definir la variable que va leer los archivos guardados en la carpeta /test/ \n",
    "streamingDF = (\n",
    "  spark\n",
    "    .readStream\n",
    "    .schema(schema)\n",
    "    .option(\"header\",\"true\")\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .csv(\"test/\")\n",
    ")\n",
    "#Declarar el evaluador que se usara para evaluar las predicciones del modelo\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"AuthorNum\",predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "\n",
    "def get_cluster(df):\n",
    "    cols=df.columns\n",
    "    cols.remove(\"AuthorNum\")\n",
    "    assembler = VectorAssembler(inputCols=cols,outputCol=\"features\")\n",
    "    data = df\n",
    "    data = assembler.transform(data)\n",
    "    \n",
    "    kmeans = KMeans().setK(12).setSeed(1)\n",
    "    model = kmeans.fit(data)\n",
    "    df_cluster = model.transform(data)\n",
    "    n = df_cluster.drop(\"features\")\n",
    "    #n.show()\n",
    "    n.write.format(\"org.neo4j.spark.DataSource\").option(\"url\",\n",
    "    \"bolt://localhost:7687\").option(\"authentication.type\",\n",
    "    \"basic\").option(\"authentication.basic.username\",\"neo4j\").option(\"authentication.basic.password\",\"unodos1234\").option(\"labels\",\n",
    "    \":Paginas\").save()\n",
    "    \n",
    "    graph.run(\"MATCH (a:Paginas) MATCH (b:Paginas) WHERE a.prediction = b.prediction MERGE (a)-[r:link]->(b)\")\n",
    "\n",
    "#Funci√≥n que se encarga de transformar cada bache y calcular las predicciones de este.\n",
    "def train_df(df,epoch_id):\n",
    "    \n",
    "    print(\"----WORKING ON BATCH----\")\n",
    "    print(\".................\")\n",
    "    print(\"# REGISTROS EN BATCH:\",df.count(),\", Atributos:\",len(df.columns))\n",
    "    get_cluster(df)\n",
    "    prediction = StreamPipeline.transform(df)\n",
    "    prediction.show()\n",
    "    dt_accuracy = evaluator.evaluate(prediction)\n",
    "    print(\"----TEST STREAMING RESULTS----\")\n",
    "    print(\"----BATCH PREDICTIONS---\")\n",
    "    print(\"Accuracy of RandomForest is = {}\" .format(dt_accuracy))\n",
    "\n",
    "query = streamingDF.writeStream.foreachBatch(train_df).start()   \n",
    "query.awaitTermination()\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
